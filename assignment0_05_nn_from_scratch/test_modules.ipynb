{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T16:44:31.670768Z",
     "start_time": "2023-06-04T16:44:31.640308Z"
    }
   },
   "outputs": [],
   "source": [
    "%run modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T16:44:31.930043Z",
     "start_time": "2023-06-04T16:44:31.927867Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T16:44:32.683453Z",
     "start_time": "2023-06-04T16:44:32.523435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_BatchNormalization (__main__.TestLayers.test_BatchNormalization) ... ok\n",
      "test_ClassNLLCriterion (__main__.TestLayers.test_ClassNLLCriterion) ... ERROR\n",
      "test_ClassNLLCriterionUnstable (__main__.TestLayers.test_ClassNLLCriterionUnstable) ... ERROR\n",
      "test_Dropout (__main__.TestLayers.test_Dropout) ... FAIL\n",
      "test_ELU (__main__.TestLayers.test_ELU) ... ERROR\n",
      "test_LeakyReLU (__main__.TestLayers.test_LeakyReLU) ... ERROR\n",
      "test_Linear (__main__.TestLayers.test_Linear) ... ok\n",
      "test_LogSoftMax (__main__.TestLayers.test_LogSoftMax) ... ok\n",
      "test_Sequential (__main__.TestLayers.test_Sequential) ... FAIL\n",
      "test_SoftMax (__main__.TestLayers.test_SoftMax) ... ok\n",
      "test_SoftPlus (__main__.TestLayers.test_SoftPlus) ... ERROR\n",
      "test_adam_optimizer (__main__.TestLayers.test_adam_optimizer) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_ClassNLLCriterion (__main__.TestLayers.test_ClassNLLCriterion)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 530, in test_ClassNLLCriterion\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_ClassNLLCriterionUnstable (__main__.TestLayers.test_ClassNLLCriterionUnstable)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 488, in test_ClassNLLCriterionUnstable\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_ELU (__main__.TestLayers.test_ELU)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 406, in test_ELU\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_LeakyReLU (__main__.TestLayers.test_LeakyReLU)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 366, in test_LeakyReLU\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_SoftPlus (__main__.TestLayers.test_SoftPlus)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 445, in test_SoftPlus\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_Dropout (__main__.TestLayers.test_Dropout)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 341, in test_Dropout\n",
      "    self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
      "AssertionError: False is not true\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_Sequential (__main__.TestLayers.test_Sequential)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 254, in test_Sequential\n",
      "    self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
      "AssertionError: False is not true\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_adam_optimizer (__main__.TestLayers.test_adam_optimizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1088332420.py\", line 551, in test_adam_optimizer\n",
      "    self.assertTrue(\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.116s\n",
      "\n",
      "FAILED (failures=3, errors=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "test5\n",
      "[[7.2007623 5.6101913 7.4758844 5.1719427 9.546602  6.2939    8.312612\n",
      "  6.5585556 7.60034   7.7335515 5.924272  9.847923  8.875664  9.6974945\n",
      "  9.474136  7.9895    9.609371  5.4424624 5.979914  5.226136  6.626652\n",
      "  6.9433866 6.3567452 9.143687  6.7837667 6.4046726 7.7134805 5.7046213\n",
      "  9.010985  5.372753  9.934435  8.861224  5.9935784 5.027611  9.077307\n",
      "  8.5342865 8.645036  8.856352  5.370223  6.792329  5.579345  9.315517\n",
      "  8.11649   6.65449   5.317792  6.5549116 6.6259165 8.648031  8.187787\n",
      "  9.436064  7.3610744 5.5979714 8.566224  8.8039255 7.806386  8.8548355\n",
      "  7.468978  7.613664  7.1377053 5.1270957 5.5394573 5.157146  8.182052\n",
      "  6.5717797 7.5428534 9.537832  6.246461  7.0519147 8.777756  6.143991\n",
      "  5.3848996 6.448757  5.8061066 9.648488  9.040602  8.167019  9.357303\n",
      "  9.01836   5.9328504 9.462795  7.696711  9.037201  9.480456  6.5900173\n",
      "  5.5502596 6.1396756 7.135539  9.090074  9.303653  5.0347605 7.5537367\n",
      "  7.087055  6.110539  5.5993266 6.688076  9.714548  6.6160145 7.593953\n",
      "  8.515095  6.818148  9.858911  9.812237  6.2589116 7.4862423 6.5043917\n",
      "  6.4242024 5.184435  8.047822  7.5133953 5.257394  6.3932323 9.541329\n",
      "  6.197809  5.7244744 7.4472637 9.928252  6.210276  8.360678  8.808098\n",
      "  6.1881876 8.641082  6.838916  8.16153   8.167648  7.6788735 5.451449\n",
      "  9.176513  6.6039004 5.9325924 5.2038755 7.954465  8.387822  5.082939\n",
      "  7.5604653 6.1324787 8.225864  5.8718324 8.454689  6.9336767 9.68365\n",
      "  5.687605  6.705332  5.5673676 9.623468  9.386697  6.289708  8.29992\n",
      "  9.086111  7.776004  7.648253  6.2092614 5.4655137 9.486079  9.50209\n",
      "  8.165507  6.695149  6.746048  8.629779  9.485551  9.435432  8.899378\n",
      "  8.210158  5.4206996 5.8081436 9.492771  8.0321455 5.045985  5.5073576\n",
      "  8.317509  5.025308  5.8040404 7.743669  8.4594755 8.259807  6.1213465\n",
      "  8.560896  6.1862454 6.6269984 8.732457  8.248164  9.246117  8.288064\n",
      "  7.841543  5.468374  6.838579  6.3260117 6.2199483 9.865053  6.9654884\n",
      "  9.460233  8.155693  8.974056  7.5131855 7.8845196 7.4625883 5.976215\n",
      "  8.612261  6.403862  5.1215796 8.227362  5.8855534 9.702293  9.769643\n",
      "  9.574322  6.8507934 5.077283  9.641593  7.1409206 9.833274  9.8181\n",
      "  9.265047  6.4722443 6.9254885 9.255683  6.58461   5.8474636 7.784006\n",
      "  9.680774  8.480149  7.850306  5.4858823 8.075036  9.95027   5.70042\n",
      "  7.591648  9.386866  8.703843  8.485079  8.512421  6.797456  6.4679594\n",
      "  9.046805  9.050567  9.3353615 9.566203  7.556712  7.5075817 8.991476\n",
      "  8.24982   8.509834  8.978963  9.4500265 6.6899757 6.877915  5.4699097\n",
      "  7.891401  5.1797113 7.32799   7.713223  6.4327064 7.9541664 5.152501\n",
      "  5.186741  9.113003  6.8009534 5.6353025 7.611216  8.849968  6.079105\n",
      "  8.114452  5.4267373 5.2584085 7.656773  7.7031755 8.187149  8.630457\n",
      "  9.87926   7.581502  6.6147823 8.975931  6.3541613 7.194857  5.392282\n",
      "  5.126754  9.813242  9.1799    8.479871  7.0447645 5.866472  5.782185\n",
      "  6.2512145 7.7461333 8.57298   8.300987  6.3996696 9.774326  8.689485\n",
      "  7.7717705 8.058603  7.0980005 6.238655  6.7798634 8.78923   5.0719676\n",
      "  5.5803633 5.2300134 5.203644  9.277303  8.51829   7.370869  5.489171\n",
      "  7.4580793 7.3673587 5.866009  7.169258  6.9925237 8.07925   8.175468\n",
      "  5.22652   6.873063  8.129299  7.5156813 9.282449  8.293468  5.814672\n",
      "  5.3528438 8.212096  5.1325564 7.928878  9.701151  7.877371  6.94085\n",
      "  8.216441  7.2912645 7.728084  9.707324  6.9305134 9.805953  9.526753\n",
      "  5.9789557 5.3468065 5.50389   5.0911093 5.4722147 8.415034  5.355943\n",
      "  6.594878  9.224377  5.1163597 9.072343  6.409274  5.590824  8.4836855\n",
      "  8.144714  9.38736   8.675355  9.017405  6.410173  5.8871975 8.753074\n",
      "  9.034174  9.952526  7.0630884 6.8600903 8.882065  6.7040176 9.653787\n",
      "  9.292064  7.14497   8.754355  8.772715  5.5156193 9.512765  7.526262\n",
      "  9.132287  6.600248  9.477616  6.946008  5.0541883 9.52691   5.4564333\n",
      "  6.596568  9.75031   9.753036  7.8671894 8.159186  7.2422276 6.466054\n",
      "  6.643323  8.362593  8.761872  8.957895  8.948091  5.4560304 7.4721017\n",
      "  5.2877936 7.7476444 7.2076526 9.438521  6.7545753 5.5853353 5.714958\n",
      "  8.807553  8.09109   5.5056133 5.420534  8.504846  5.363815  9.109301\n",
      "  8.531211  5.406744  5.4241886 9.933198  6.871354  6.853211  9.063998\n",
      "  9.736243  9.930005  8.766891  6.881298  5.4175034 8.885735  7.7920213\n",
      "  7.12111   9.531772  5.5559874 7.4631257 5.0567684 7.343303  5.2815166\n",
      "  5.5940895 5.587631  8.246052  8.730225  7.916844  9.810863  6.874353\n",
      "  6.4285603 9.342996  6.117979  9.8161125 5.0607724 9.849394  5.2157993\n",
      "  9.455715  7.6385055 9.964824  5.368983  7.7692714 9.846513  7.615489\n",
      "  8.146994  8.478744  7.2727056 8.137791  7.9215717 9.50579   5.227232\n",
      "  6.404816  9.752057  9.451319  7.2782836 8.100663  6.386906  5.9406056\n",
      "  7.318492  6.7667613 7.9182806 5.3886733 9.871974  9.931054  8.4908085\n",
      "  7.680482  6.547638  9.068975  8.4236555 5.8130846 9.554636  9.112686\n",
      "  9.749     8.628597  8.067076  7.091215  9.663643  9.330319  5.2260933\n",
      "  5.131835  6.882317  9.052767  9.93638   5.7520843 7.9706535 6.904454\n",
      "  9.849572  9.210594  9.191644  7.343466  7.0740976 6.3670354 5.2818775\n",
      "  9.323612  9.064505  9.998589  9.983184  7.7771587 8.844937  9.723828\n",
      "  9.248237  6.2367406 7.252721  5.6457973 9.770255  8.030873  6.143214\n",
      "  8.358503  8.090641  6.7908134 5.567788  8.357866  7.6015387 8.861592\n",
      "  7.6008177 9.260907  7.7595344 7.80469   9.383268  7.017414  5.6700764\n",
      "  5.1439133 8.775686  8.101548  8.520399  6.064821  5.6818576 5.0727234\n",
      "  6.752938  7.9495883 6.9612203 7.1873746 9.520794  6.741277  7.5699472\n",
      "  8.918265  6.9827137 8.110434  9.311818  9.747603  5.7353673 9.632938\n",
      "  7.4605813 6.291222  7.2956786 9.900163  7.4630904 6.643758  8.167005\n",
      "  6.200728  5.379317  5.6443987 5.640229  5.7595134 5.6941357 8.204373\n",
      "  5.9094005 6.7283363 9.483942  7.369808  8.337789  5.8615994 5.9614453\n",
      "  5.2043433 5.8446755 6.3929515 5.885052  5.4435124 5.6031795 7.303894\n",
      "  6.0316687 6.821349  7.5170865 8.451974  5.196561  8.997052  8.139502\n",
      "  5.4087954 9.367893  9.604362  5.30539   6.3843884 9.031007  8.741299\n",
      "  5.922605  6.0467467 6.8523607 7.422615  8.091274  6.8445683 7.3126736\n",
      "  8.737354  5.183416  6.2621846 8.566748  9.476034  7.5583873 7.6605673\n",
      "  5.53586   7.237062  7.6630864 6.2123528 6.346216  6.8864207 5.100356\n",
      "  6.610396  6.05724   6.637487  5.5988107 9.452637  7.9679623 8.395512\n",
      "  8.945856  7.492211  5.4346013 7.6855326 7.9342055 8.727198  7.1582975\n",
      "  5.6379013 6.4188795 6.8154116 8.229587  7.8538914 6.7804837 9.932576\n",
      "  8.028874  6.186134  5.5089126 5.7642956 6.229789  5.8034067 5.932835\n",
      "  6.425476  5.866868  9.483828  5.401169  7.622557  7.0519843 9.911893\n",
      "  5.5601945 6.989278  9.847352  9.327536  9.085361  6.289514  5.854438\n",
      "  8.343216  9.64688   7.7838144 7.858063  6.3998957 8.847465  5.935219\n",
      "  6.6183963 7.127182  7.538052  6.2120485 5.574184  8.053101  6.443153\n",
      "  7.906191  5.7718134 7.4057007 7.662947  5.2591176 6.6830215 5.6720734\n",
      "  5.316875  9.949801  6.611769  9.049373  6.2732034 8.407514  8.801139\n",
      "  7.9781938 7.357881  7.0592046 6.7443414 9.647646  9.153097  9.825134\n",
      "  5.621486  8.654337  9.691702  5.906165  5.3324814 8.705604  7.8723655\n",
      "  9.209144  5.698862  8.9763365 6.0081367 5.8182797 5.821329  9.072874\n",
      "  8.325986  7.6153274 6.7941523 9.386003  6.9622254 9.082997  7.1956744\n",
      "  6.884722  7.313399  6.5068893 8.738047  7.513602  6.1610637 9.497873\n",
      "  6.919456  7.7177644 9.532361  8.12119   5.5844903 9.699161  8.13854\n",
      "  6.674528  5.6963606 8.970126  8.100364  7.6673055 9.469463  8.9429865\n",
      "  5.758374  6.5586104 6.2424455 8.719731  5.167662  7.849448  8.812293\n",
      "  9.383828  6.7104087 9.106286  5.5531588 9.232262  5.6374435 6.9864364\n",
      "  8.986477  5.749587  6.146257  8.611263  8.600183  8.205738  8.469742\n",
      "  7.713622  6.258995  6.72848   5.9079885 9.542253  7.916959  7.004257\n",
      "  7.310029  9.736417  5.766757  7.931149  7.5294433 8.057271  5.090551\n",
      "  9.36062   9.660591  7.825666  8.483254  9.612497  8.536193  5.7626953\n",
      "  7.8814416 8.033575  7.120653  8.682221  9.671835  9.627843  7.2541966\n",
      "  5.5661902 9.924206  9.19449   5.6233134 9.604209  9.349482  7.59419\n",
      "  7.956377  6.9950137 5.273808  6.6759863 9.014267  5.02316   6.6674957\n",
      "  6.9908433 7.686978  9.599278  6.73173   6.734766  8.687507  7.26109\n",
      "  6.123024  7.2621975 5.704285  5.881935  7.491839  7.0946274 9.574229\n",
      "  6.8119693 7.9029417 8.161322  5.065472  8.317687  5.8901796 9.805351\n",
      "  5.743314  7.0731206 5.4267483 9.984371  7.510975  7.976925  5.3353825\n",
      "  8.749803  6.049528  9.490272  6.025698  5.9534388 5.1827483 7.360335\n",
      "  7.824206  5.328543  8.877638  7.266444  7.6219516 7.2038136 7.003815\n",
      "  7.7982016 5.7762012 5.909641  9.3089285 9.730577  6.8665466 6.3537235\n",
      "  8.219997  7.0436707 5.1269317 5.780763  8.579861  8.29462   5.13548\n",
      "  6.109861  6.155374  8.359464  5.0985527 5.520543  8.99958   5.892723\n",
      "  8.263731  6.1909137 5.497207  6.215861  8.611335  9.278482  9.151099\n",
      "  6.9859176 8.3404255 6.0249214 6.465739  9.481679  5.0650096 5.4275427\n",
      "  6.039431  5.132661  5.907177  7.915208  7.107123  9.463359  9.087218\n",
      "  6.709087  6.297117  6.898462  7.9514747 6.340318  8.120745  7.047058\n",
      "  7.760236  7.1806326 6.4723287 9.742267  8.818029  5.700566  9.3423395\n",
      "  7.437156  9.472761  8.999276  7.1260676 5.1123466 6.3433867 7.708171\n",
      "  8.167391  6.2894382 5.69678   9.174651  9.922011  7.628451  5.8583965\n",
      "  6.3615365 5.0919533 9.571494  5.5887556 7.882582  6.370276  7.77089\n",
      "  8.257102  9.148709  6.0321064 5.0549793 5.684428  9.500093  9.369451\n",
      "  7.9870653 8.002584  8.325183  5.8768563 9.57206   7.0938525 6.915693\n",
      "  7.5945888 5.23483   5.8314166 8.690168  5.4139934 8.01576   6.2267456\n",
      "  6.946478  6.4434686 6.7783637 8.595229  6.4856086 7.832023  7.380252\n",
      "  8.318356  9.684149  8.662861  6.074702  5.1559157 6.3113203 7.9753895\n",
      "  5.257129  7.481831  7.9842143 6.6712193 8.854561  5.5329914 5.375689\n",
      "  8.640944  7.4774566 8.442012  7.1741366 6.2320104 9.095511  8.99708\n",
      "  8.473482  6.360726  7.9511533 6.8048697 5.4579105 9.586568  5.684093\n",
      "  9.751186  7.2300286 5.9256644 7.7095046 9.364729  8.661124  9.032805\n",
      "  8.293917  8.461383  9.245978  6.24834   7.447125  6.106047  9.93834\n",
      "  9.720297  5.197134  8.527876  9.626242  5.902877  7.839726  9.577441\n",
      "  5.1697297 8.487102  6.486745  9.621981  9.855291  9.721333  7.371071\n",
      "  9.310213  9.222747  6.5955024 9.144577  5.185038  7.9813495 6.150044\n",
      "  5.602834  5.384766  8.481443  6.6993747 8.623834  5.3267817]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=5 failures=3>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLayers(unittest.TestCase):\n",
    "    def test_Linear(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
    "            custom_layer = Linear(n_in, n_out)\n",
    "            custom_layer.W = torch_layer.weight.data.numpy()\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_SoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softmax(dim=1)\n",
    "            custom_layer = SoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "            next_layer_grad = next_layer_grad.clip(1e-5, 1.0)\n",
    "            next_layer_grad = 1.0 / next_layer_grad\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_LogSoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "            custom_layer = LogSoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_BatchNormalization(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 32, 16\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            alpha = 0.9\n",
    "            custom_layer = BatchNormalization(alpha)\n",
    "            custom_layer.train()\n",
    "            torch_layer = torch.nn.BatchNorm1d(\n",
    "                n_in, eps=custom_layer.EPS, momentum=1.0 - alpha, affine=False\n",
    "            )\n",
    "            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            # please, don't increase `atol` parameter, it's garanteed that you can implement batch norm layer\n",
    "            # with tolerance 1e-5\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 3. check moving mean\n",
    "            self.assertTrue(\n",
    "                np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy())\n",
    "            )\n",
    "            # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "            # self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "            # 4. check evaluation mode\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.evaluate()\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            torch_layer.eval()\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_Sequential(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 0.9\n",
    "            torch_layer = torch.nn.BatchNorm1d(\n",
    "                n_in, eps=BatchNormalization.EPS, momentum=1.0 - alpha, affine=True\n",
    "            )\n",
    "            torch_layer.bias.data = torch.from_numpy(\n",
    "                np.random.random(n_in).astype(np.float32)\n",
    "            )\n",
    "            custom_layer = Sequential()\n",
    "            bn_layer = BatchNormalization(alpha)\n",
    "            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            custom_layer.add(bn_layer)\n",
    "            scaling_layer = ChannelwiseScaling(n_in)\n",
    "            scaling_layer.gamma = torch_layer.weight.data.numpy()\n",
    "            scaling_layer.beta = torch_layer.bias.data.numpy()\n",
    "            custom_layer.add(scaling_layer)\n",
    "            custom_layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_Dropout(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            p = np.random.uniform(0.3, 0.7)\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(\n",
    "                np.all(\n",
    "                    np.logical_or(\n",
    "                        np.isclose(layer_output, 0),\n",
    "                        np.isclose(layer_output * (1.0 - p), layer_input),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(\n",
    "                np.all(\n",
    "                    np.logical_or(\n",
    "                        np.isclose(layer_grad, 0),\n",
    "                        np.isclose(layer_grad * (1.0 - p), next_layer_grad),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 3. check evaluation mode\n",
    "            layer.evaluate()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            print(np.max(np.abs(layer_output - layer_input)))\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            # 4. check mask\n",
    "            p = 0.0\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.allclose(layer_output, layer_input))\n",
    "\n",
    "            p = 0.5\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n",
    "\n",
    "            # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "            batch_size, n_in = 1000, 1\n",
    "            p = 0.8\n",
    "            layer = Dropout(p)\n",
    "            layer.train()\n",
    "\n",
    "            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "            layer_input = layer_input.T\n",
    "            layer_output = layer.updateOutput(layer_input)\n",
    "            print('test5')\n",
    "            print(layer_input.shape)\n",
    "            print('------')\n",
    "            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "    def test_LeakyReLU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            slope = np.random.uniform(0.01, 0.05)\n",
    "            torch_layer = torch.nn.LeakyReLU(slope)\n",
    "            custom_layer = LeakyReLU(slope)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_ELU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            alpha = 1.0\n",
    "            torch_layer = torch.nn.ELU(alpha)\n",
    "            custom_layer = ELU(alpha)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_SoftPlus(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Softplus()\n",
    "            custom_layer = SoftPlus()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_ClassNLLCriterionUnstable(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "            layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n",
    "            layer_input /= layer_input.sum(axis=-1, keepdims=True)\n",
    "            layer_input = layer_input.clip(\n",
    "                custom_layer.EPS, 1.0 - custom_layer.EPS\n",
    "            )  # unifies input\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(\n",
    "                torch.log(layer_input_var), torch.from_numpy(target_labels)\n",
    "            )\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_ClassNLLCriterion(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.NLLLoss()\n",
    "            custom_layer = ClassNLLCriterion()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            layer_input = torch.nn.LogSoftmax(dim=1)(\n",
    "                torch.from_numpy(layer_input)\n",
    "            ).data.numpy()\n",
    "            target_labels = np.random.choice(n_in, batch_size)\n",
    "            target = np.zeros((batch_size, n_in), np.float32)\n",
    "            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(\n",
    "                layer_input_var, torch.from_numpy(target_labels)\n",
    "            )\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "            torch_layer_output_var.backward()\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test_adam_optimizer(self):\n",
    "        state = {}\n",
    "        config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "        variables = [[np.arange(10).astype(np.float64)]]\n",
    "        gradients = [[np.arange(10).astype(np.float64)]]\n",
    "        adam_optimizer(variables, gradients, config, state)\n",
    "        self.assertTrue(\n",
    "            np.allclose(\n",
    "                state['m'][0],\n",
    "                np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "            )\n",
    "        )\n",
    "        self.assertTrue(\n",
    "            np.allclose(\n",
    "                state['v'][0],\n",
    "                np.array(\n",
    "                    [0.0, 0.001, 0.004, 0.009, 0.016, 0.025, 0.036, 0.049, 0.064, 0.081]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        self.assertTrue(state['t'] == 1)\n",
    "        self.assertTrue(\n",
    "            np.allclose(\n",
    "                variables[0][0],\n",
    "                np.array(\n",
    "                    [0.0, 0.999, 1.999, 2.999, 3.999, 4.999, 5.999, 6.999, 7.999, 8.999]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        adam_optimizer(variables, gradients, config, state)\n",
    "        self.assertTrue(\n",
    "            np.allclose(\n",
    "                state['m'][0],\n",
    "                np.array([0.0, 0.19, 0.38, 0.57, 0.76, 0.95, 1.14, 1.33, 1.52, 1.71]),\n",
    "            )\n",
    "        )\n",
    "        self.assertTrue(\n",
    "            np.allclose(\n",
    "                state['v'][0],\n",
    "                np.array(\n",
    "                    [\n",
    "                        0.0,\n",
    "                        0.001999,\n",
    "                        0.007996,\n",
    "                        0.017991,\n",
    "                        0.031984,\n",
    "                        0.049975,\n",
    "                        0.071964,\n",
    "                        0.097951,\n",
    "                        0.127936,\n",
    "                        0.161919,\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        self.assertTrue(state['t'] == 2)\n",
    "        self.assertTrue(\n",
    "            np.allclose(\n",
    "                variables[0][0],\n",
    "                np.array(\n",
    "                    [0.0, 0.998, 1.998, 2.998, 3.998, 4.998, 5.998, 6.998, 7.998, 8.998]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T15:07:38.037730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_Conv2d (__main__.TestAdvancedLayers.test_Conv2d) ... ERROR\n",
      "test_MaxPool2d (__main__.TestAdvancedLayers.test_MaxPool2d) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_Conv2d (__main__.TestAdvancedLayers.test_Conv2d)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1999895574.py\", line 30, in test_Conv2d\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_MaxPool2d (__main__.TestAdvancedLayers.test_MaxPool2d)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g9/qd_50t6125vb7v8pbdy0x1rh0000gn/T/ipykernel_9741/1999895574.py\", line 87, in test_MaxPool2d\n",
      "    np.allclose(\n",
      "  File \"<__array_function__ internals>\", line 200, in allclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2270, in allclose\n",
      "    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<__array_function__ internals>\", line 200, in isclose\n",
      "  File \"/Users/pavelkurach/VSCodeProjects/msai-ml/venv/lib/python3.11/site-packages/numpy/core/numeric.py\", line 2378, in isclose\n",
      "    yfin = isfinite(y)\n",
      "           ^^^^^^^^^^^\n",
      "TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.007s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=2 failures=0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestAdvancedLayers(unittest.TestCase):\n",
    "    def test_Conv2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        h, w = 5, 6\n",
    "        kern_size = 3\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Conv2d(n_in, n_out, kern_size, padding=1)\n",
    "            custom_layer = Conv2d(n_in, n_out, kern_size)\n",
    "            custom_layer.W = (\n",
    "                torch_layer.weight.data.numpy()\n",
    "            )  # [n_out, n_in, kern, kern]\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-1, 1, (batch_size, n_in, h, w)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(\n",
    "                -1, 1, (batch_size, n_out, h, w)\n",
    "            ).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            # m = ~np.isclose(torch_weight_grad, weight_grad, atol=1e-5)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_weight_grad,\n",
    "                    weight_grad,\n",
    "                    atol=1e-6,\n",
    "                )\n",
    "            )\n",
    "            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n",
    "\n",
    "    def test_MaxPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 3\n",
    "        h, w = 4, 6\n",
    "        kern_size = 2\n",
    "        for _ in range(100):\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.MaxPool2d(kern_size)\n",
    "            custom_layer = MaxPool2d(kern_size)\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in, h, w)).astype(\n",
    "                np.float32\n",
    "            )\n",
    "            next_layer_grad = np.random.uniform(\n",
    "                -10, 10, (batch_size, n_in, h // kern_size, w // kern_size)\n",
    "            ).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = torch.from_numpy(layer_input).requires_grad_(True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(\n",
    "                layer_input, next_layer_grad\n",
    "            )\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertTrue(\n",
    "                np.allclose(\n",
    "                    torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestAdvancedLayers)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T15:07:38.038574Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
